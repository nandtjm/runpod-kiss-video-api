# Production Dockerfile - Pre-loaded Volume Strategy
# Used by professional AI services: ComfyUI Cloud, Automatic1111, etc.
# Models stored on RunPod Network Volume, shared across all containers

FROM runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    unzip \
    ffmpeg \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    libgoogle-perftools4 \
    libtcmalloc-minimal4 \
    && rm -rf /var/lib/apt/lists/*

# Set environment variables for pre-loaded volume
ENV PYTHONPATH=/app
ENV CUDA_HOME=/usr/local/cuda
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
ENV PYTHONUNBUFFERED=1

# Volume-based model paths (models pre-loaded on RunPod Network Volume)
ENV MODEL_CACHE_DIR=/workspace/models
ENV TEMP_DIR=/tmp
ENV HUGGINGFACE_HUB_CACHE=/workspace/models/.cache

# Copy requirements and install Python dependencies
COPY requirements.txt ./
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Install PyTorch with CUDA support (ensure latest)
RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install additional dependencies
RUN pip install huggingface_hub transformers runpod

# Copy application code
COPY . .

# Create model validation script
RUN echo '#!/bin/bash\n\
echo "ðŸ” Validating Pre-loaded Models on Volume..."\n\
echo "Volume mount point: $MODEL_CACHE_DIR"\n\
\n\
# Check if volume is mounted\n\
if [ ! -d "$MODEL_CACHE_DIR" ]; then\n\
    echo "âŒ ERROR: Network volume not mounted at $MODEL_CACHE_DIR"\n\
    echo "   Please ensure RunPod Network Volume is attached to /workspace"\n\
    exit 1\n\
fi\n\
\n\
# Check Wan-AI model\n\
WAN_MODEL_PATH="$MODEL_CACHE_DIR/Wan2.1-I2V-14B-720P"\n\
if [ -d "$WAN_MODEL_PATH" ] && [ "$(ls -A $WAN_MODEL_PATH)" ]; then\n\
    MODEL_SIZE=$(du -sh "$WAN_MODEL_PATH" | cut -f1)\n\
    FILE_COUNT=$(find "$WAN_MODEL_PATH" -type f | wc -l)\n\
    echo "âœ… Wan-AI model found: $MODEL_SIZE, $FILE_COUNT files"\n\
    \n\
    # Verify critical files exist\n\
    if ls "$WAN_MODEL_PATH"/*.safetensors >/dev/null 2>&1; then\n\
        echo "âœ… Model weights found (.safetensors files)"\n\
    elif ls "$WAN_MODEL_PATH"/*.bin >/dev/null 2>&1; then\n\
        echo "âœ… Model weights found (.bin files)"\n\
    elif ls "$WAN_MODEL_PATH"/*.pth >/dev/null 2>&1; then\n\
        echo "âœ… Model weights found (.pth files)"\n\
    else\n\
        echo "âš ï¸  Model directory exists but no weight files found"\n\
    fi\n\
else\n\
    echo "âŒ Wan-AI model NOT found at $WAN_MODEL_PATH"\n\
    echo "   Please run setup script to pre-load models on volume"\n\
    echo "   Expected path: $WAN_MODEL_PATH"\n\
    exit 1\n\
fi\n\
\n\
# Check LoRA model\n\
LORA_MODEL_PATH="$MODEL_CACHE_DIR/kissing-lora"\n\
if [ -d "$LORA_MODEL_PATH" ] && [ "$(ls -A $LORA_MODEL_PATH)" ]; then\n\
    LORA_SIZE=$(du -sh "$LORA_MODEL_PATH" | cut -f1)\n\
    echo "âœ… LoRA model found: $LORA_SIZE"\n\
else\n\
    echo "âš ï¸  LoRA model not found (optional)"\n\
fi\n\
\n\
echo "âœ… Model validation complete - starting API server"\n\
echo "ðŸš€ All models loaded from pre-loaded volume"\n\
python3 rp_handler.py\n\
' > /app/start_with_volume.sh && chmod +x /app/start_with_volume.sh

# Create health check that validates models
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8000/health && \
      test -d "/workspace/models/Wan2.1-I2V-14B-720P" || exit 1

# Expose port
EXPOSE 8000

# Start with model validation
CMD ["/app/start_with_volume.sh"]