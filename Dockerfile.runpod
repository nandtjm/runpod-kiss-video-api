# RunPod Serverless Dockerfile - Uses Network Storage for Models
# Strategy: Download models once to RunPod Network Storage, reuse across all containers
FROM runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    unzip \
    ffmpeg \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    libgoogle-perftools4 \
    libtcmalloc-minimal4 \
    && rm -rf /var/lib/apt/lists/*

# Set environment variables for RunPod
ENV PYTHONPATH=/app
ENV CUDA_HOME=/usr/local/cuda
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
ENV PYTHONUNBUFFERED=1

# RunPod Network Storage paths (persistent across containers)
ENV RUNPOD_VOLUME_PATH=/runpod-volume
ENV MODEL_CACHE_DIR=/runpod-volume/models
ENV TEMP_DIR=/tmp
ENV HUGGINGFACE_HUB_CACHE=/runpod-volume/models/.cache

# Copy requirements and install Python dependencies
COPY requirements.txt ./
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Install PyTorch with CUDA support (already in base image, but ensure latest)
RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install model download dependencies
RUN pip install huggingface_hub transformers runpod

# Copy application code
COPY . .

# Create model initialization script for RunPod Network Storage
RUN echo '#!/bin/bash\n\
echo "🔍 Checking RunPod Network Storage for models..."\n\
\n\
# Create model directories in network storage if they dont exist\n\
mkdir -p "$MODEL_CACHE_DIR" "$HUGGINGFACE_HUB_CACHE"\n\
\n\
# Check if models exist in network storage\n\
WAN_MODEL_PATH="$MODEL_CACHE_DIR/Wan2.1-I2V-14B-720P"\n\
LORA_MODEL_PATH="$MODEL_CACHE_DIR/kissing-lora"\n\
\n\
if [ -d "$WAN_MODEL_PATH" ] && [ "$(ls -A $WAN_MODEL_PATH)" ]; then\n\
    echo "✅ Wan-AI model found in network storage ($WAN_MODEL_PATH)"\n\
    MODEL_SIZE=$(du -sh "$WAN_MODEL_PATH" | cut -f1)\n\
    echo "   Model size: $MODEL_SIZE"\n\
else\n\
    echo "📥 Downloading Wan-AI model to network storage..."\n\
    echo "   This is a one-time download (~28GB)"\n\
    echo "   Future containers will reuse this model"\n\
    \n\
    python3 -c "\n\
import os\n\
from huggingface_hub import snapshot_download\n\
try:\n\
    snapshot_download(\n\
        repo_id='"'"'Wan-AI/Wan2.1-I2V-14B-720P'"'"',\n\
        local_dir='"'"'$WAN_MODEL_PATH'"'"',\n\
        resume_download=True\n\
    )\n\
    print('"'"'✅ Wan-AI model downloaded successfully'"'"')\n\
except Exception as e:\n\
    print(f'"'"'❌ Failed to download Wan-AI model: {e}'"'"')\n\
    print('"'"'⚠️  Container will start but model generation will fail'"'"')\n\
"\n\
fi\n\
\n\
if [ -d "$LORA_MODEL_PATH" ] && [ "$(ls -A $LORA_MODEL_PATH)" ]; then\n\
    echo "✅ LoRA model found in network storage"\n\
else\n\
    echo "📥 Downloading LoRA model..."\n\
    python3 -c "\n\
import os\n\
from huggingface_hub import snapshot_download\n\
try:\n\
    snapshot_download(\n\
        repo_id='"'"'Remade-AI/kissing-lora'"'"',\n\
        local_dir='"'"'$LORA_MODEL_PATH'"'"',\n\
        resume_download=True\n\
    )\n\
    print('"'"'✅ LoRA model downloaded successfully'"'"')\n\
except Exception as e:\n\
    print(f'"'"'❌ Failed to download LoRA model: {e}'"'"')\n\
"\n\
fi\n\
\n\
echo "🚀 Starting Kiss Video Generator API..."\n\
python3 rp_handler.py\n\
' > /app/start_runpod.sh && chmod +x /app/start_runpod.sh

# Expose port for RunPod
EXPOSE 8000

# RunPod handler
CMD ["/app/start_runpod.sh"]