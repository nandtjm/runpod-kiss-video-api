# RunPod Serverless Dockerfile - Latest CUDA 12.8 & PyTorch 2.8+
# Updated for newest RunPod requirements

FROM runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

WORKDIR /app

# Install system dependencies for video processing
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Environment variables for RunPod serverless with latest CUDA
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV MODEL_CACHE_DIR=/runpod-volume/models
ENV TEMP_DIR=/tmp
ENV CUDA_HOME=/usr/local/cuda
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# Copy requirements first for better caching
COPY requirements_latest.txt ./requirements.txt
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# Install latest PyTorch 2.8+ with CUDA 12.8 support
RUN pip install --no-cache-dir --pre torch>=2.8.0 torchvision>=0.21.0 torchaudio>=2.8.0 \
    --index-url https://download.pytorch.org/whl/nightly/cu124 || \
    pip install --no-cache-dir torch>=2.5.0 torchvision>=0.20.0 torchaudio>=2.5.0 \
    --index-url https://download.pytorch.org/whl/cu124

# Install remaining requirements
RUN pip install --no-cache-dir -r requirements.txt

# Copy application files
COPY handler.py ./
COPY runpod_serverless.py ./

# Create startup validation script with latest CUDA checks
RUN echo '#!/bin/bash\n\
echo "ðŸ” RunPod Serverless Startup Validation (Latest CUDA/PyTorch)"\n\
echo "============================================================"\n\
\n\
# Check Python and key imports\n\
python3 -c "import sys; print(f\"Python: {sys.version}\")" || exit 1\n\
\n\
# Check CUDA availability and version\n\
echo "ðŸ”¥ CUDA & PyTorch Check:"\n\
python3 -c "\n\
import torch\n\
import sys\n\
print(f\"PyTorch: {torch.__version__}\")\n\
print(f\"CUDA available: {torch.cuda.is_available()}\")\n\
if torch.cuda.is_available():\n\
    print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n\
    print(f\"GPU count: {torch.cuda.device_count()}\")\n\
    if torch.cuda.device_count() > 0:\n\
        print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n\
        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n\
    # Check minimum PyTorch version\n\
    major, minor = map(int, torch.__version__.split(\".\")[:2])\n\
    if major > 2 or (major == 2 and minor >= 5):\n\
        print(\"âœ… PyTorch version meets requirements\")\n\
    else:\n\
        print(f\"âš ï¸  PyTorch {torch.__version__} - consider upgrading to 2.5+\")\n\
else:\n\
    print(\"âŒ CUDA not available - GPU operations will fail!\")\n\
    # Don\"t exit - might be testing without GPU\n\
" || exit 1\n\
\n\
# Check other critical imports\n\
python3 -c "import cv2; print(f\"OpenCV: {cv2.__version__}\")" || exit 1\n\
python3 -c "import runpod; print(f\"RunPod: Available\")" || exit 1\n\
python3 -c "import numpy as np; print(f\"NumPy: {np.__version__}\")" || exit 1\n\
\n\
# Check volume mount points\n\
echo "ðŸ“ Volume Mount Check:"\n\
echo "  /runpod-volume exists: $(test -d /runpod-volume && echo \"âœ…\" || echo \"âŒ\")"\n\
echo "  /workspace exists: $(test -d /workspace && echo \"âœ…\" || echo \"âŒ\")"\n\
\n\
# List volume contents if available\n\
if [ -d "/runpod-volume" ]; then\n\
    VOLUME_CONTENTS=$(ls -la /runpod-volume 2>/dev/null || echo "Cannot list")\n\
    echo "  /runpod-volume contents: $VOLUME_CONTENTS"\n\
    \n\
    # Check models specifically\n\
    if [ -d "/runpod-volume/models" ]; then\n\
        echo "  Models directory found: âœ…"\n\
        MODEL_COUNT=$(find /runpod-volume/models -mindepth 1 -maxdepth 1 -type d 2>/dev/null | wc -l || echo "0")\n\
        echo "  Model directories: $MODEL_COUNT"\n\
    else\n\
        echo "  Models directory: âŒ Not found at /runpod-volume/models"\n\
    fi\n\
fi\n\
\n\
# Test handler import\n\
echo "ðŸ§ª Handler Import Test:"\n\
python3 -c "from handler import handler; print(\"âœ… Handler import successful\")" || {\n\
    echo "âŒ Handler import failed"\n\
    python3 -c "import sys; print(f\"Python path: {sys.path}\")" \n\
    ls -la /app/\n\
    exit 1\n\
}\n\
\n\
echo "âœ… All startup validations passed!"\n\
echo "ðŸš€ Latest CUDA/PyTorch environment ready"\n\
echo "ðŸŽ¯ Starting RunPod serverless worker..."\n\
\n\
# Start the actual service\n\
python3 runpod_serverless.py\n\
' > /app/start.sh && chmod +x /app/start.sh

# Health check that works with RunPod serverless
HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
  CMD python3 -c "from handler import health_check; h=health_check(); exit(0 if h.get('status')=='healthy' else 1)" || exit 1

# Expose port (RunPod handles this automatically)
EXPOSE 8000

# Use the startup script
CMD ["/app/start.sh"]