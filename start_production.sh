#!/bin/bash

echo "üöÄ Production AI Kiss Video Generator"
echo "===================================="
echo ""

# Environment info
echo "üìã Environment Information:"
echo "  Python: $(python3 --version)"
echo "  Working Directory: $(pwd)"
echo "  Model Cache: ${MODEL_CACHE_DIR}"
echo "  Device: ${CUDA_VISIBLE_DEVICES:-auto}"
echo ""

# GPU validation
echo "üéÆ GPU Validation:"
if command -v nvidia-smi >/dev/null 2>&1; then
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits | while IFS=, read gpu_name memory_total memory_free; do
        echo "  GPU: $gpu_name"
        echo "  VRAM: ${memory_free}MB free / ${memory_total}MB total"
    done
else
    echo "  ‚ö†Ô∏è  nvidia-smi not available"
fi
echo ""

# PyTorch validation
echo "üî• PyTorch Validation:"
python3 -c "
import torch
print(f'  PyTorch: {torch.__version__}')
print(f'  CUDA Available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'  CUDA Version: {torch.version.cuda}')
    print(f'  GPU Count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')
" || echo "  ‚ùå PyTorch validation failed"
echo ""

# Volume validation
echo "üìÅ Volume Validation:"
if [ -d "/runpod-volume" ]; then
    echo "  ‚úÖ Volume mounted at /runpod-volume"
    volume_size=$(df -h /runpod-volume | awk 'NR==2 {print $2}')
    volume_used=$(df -h /runpod-volume | awk 'NR==2 {print $3}')
    echo "  Volume Size: ${volume_used} used / ${volume_size} total"
    
    if [ -d "${MODEL_CACHE_DIR}" ]; then
        echo "  ‚úÖ Models directory found"
        model_count=$(find "${MODEL_CACHE_DIR}" -type f 2>/dev/null | wc -l)
        echo "  Model files: ${model_count}"
        
        # Check Wan-AI model specifically
        wan_model_path="${MODEL_CACHE_DIR}/Wan2.1-I2V-14B-720P"
        if [ -d "$wan_model_path" ]; then
            wan_files=$(find "$wan_model_path" -type f 2>/dev/null | wc -l)
            echo "  ‚úÖ Wan-AI model: ${wan_files} files"
        else
            echo "  ‚ùå Wan-AI model not found"
        fi
    else
        echo "  ‚ùå Models directory not found: ${MODEL_CACHE_DIR}"
    fi
else
    echo "  ‚ùå Volume not mounted"
fi
echo ""

# Dependencies validation
echo "üîß Dependencies Validation:"
python3 -c "
import sys
required_packages = [
    'runpod', 'torch', 'diffusers', 'transformers', 
    'PIL', 'cv2', 'numpy', 'loguru'
]

for package in required_packages:
    try:
        if package == 'PIL':
            import PIL
            version = PIL.__version__
        elif package == 'cv2':
            import cv2
            version = cv2.__version__
        else:
            module = __import__(package)
            version = getattr(module, '__version__', 'unknown')
        print(f'  ‚úÖ {package}: {version}')
    except ImportError:
        print(f'  ‚ùå {package}: not found')
        sys.exit(1)
" || echo "  ‚ùå Dependencies validation failed"
echo ""

# Memory optimization
echo "üß† Memory Optimization:"
python3 -c "
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print('  ‚úÖ GPU memory cleared')
    print('  ‚úÖ CUDA optimizations enabled')
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
else:
    print('  ‚ö†Ô∏è  CUDA not available, using CPU')
" || echo "  ‚ùå Memory optimization failed"
echo ""

# Final health check
echo "üè• Production Health Check:"
python3 -c "
from handler import health_check
import json
result = health_check()
print(json.dumps(result, indent=2))
if result.get('status') != 'healthy':
    print('‚ùå Health check failed!')
    exit(1)
else:
    print('‚úÖ All systems ready!')
" || echo "‚ùå Health check script failed"
echo ""

# Start the serverless handler
echo "‚úÖ Starting production serverless handler..."
echo ""
python3 runpod_serverless.py